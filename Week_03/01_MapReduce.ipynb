{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce\n",
    "\n",
    "Eerst maken we een aparte directory aan voor alles wat we voor deze notebook gaan gebruiken in hdfs. Dit om conflicten of het overschrijven van gegevens te vermijden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Hello World,\n",
      "hello world,\n",
      "hello world,\n",
      "\n",
      "Dit is een voorbeeld file om het Wordcount voorbeeld te testen !\n"
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "map = 'MapReduce'\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    # do some cleaning in case anything else than *.txt is present\n",
    "    for f in client.list(map):\n",
    "        client.delete(map + '/' + f, recursive=True)\n",
    "\n",
    "client.upload(map, 'input.txt')\n",
    "client.upload(map, 'titanic.csv')\n",
    "with client.read(map + '/input.txt') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wat is MapReduce\n",
    "\n",
    "MapReduce is een programmeermodel om eenvoudig distributed data te verwerken.\n",
    "Het is belangrijk om te realiseren dat de programma's die je hier schrijft een parallel uitgevoerd worden op verschillende stukjes data (De map-fase) om daarna in de reduce-fase tot een finale output teruggebracht te worden.\n",
    "Er gebeuren 5 stappen bij het uitvoeren van een MapReduce programma\n",
    "* Bepalen op welke nodes de code uitgevoerd wordt (wordt door YARN gedaan afhankelijk van de locatie van de blocks)\n",
    "* Uitvoeren van de Map-code (Geschreven door de developer naar eigen wens)\n",
    "* Shuffle, ouput van de map-fase doorsturen naar andere nodes die de resultaten gaan reduceren (Automatisch)\n",
    "* uitvoeren van de Reduce-code (Geschreven door de developer naar eigen wens)\n",
    "* Combineren van de reduce output tot 1 gehele/finale output (Automatisch)\n",
    "\n",
    "Uit bovenstaand stappenplan is het duidelijk dat er twee zaken moeten geimplementeerd worden bij het schrijven van een MapReduce toepassing.\n",
    "Echter zullen we eerst een aantal voorbeelden bestuderen met reeds bestaande implementaties om zo meer vertrouwd te geraken met de flow van MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Voorbeelden van bestaande applicaties\n",
    "\n",
    "Reeds een aantal default MapReduce applications zijn mee geinstalleerd met Hadoop.\n",
    "De jar die deze toepassingen bevat kan gevonden worden in hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar.\n",
    "Wanneer je deze jar uitvoert met onderstaande commando krijg je een lijst met de beschikbare applicaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example program must be given as the first argument.\n",
      "Valid program names are:\n",
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
      "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
      "  dbcount: An example job that count the pageview counts from a database.\n",
      "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
      "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
      "  join: A job that effects a join over sorted, equally partitioned datasets\n",
      "  multifilewc: A job that counts words from several files.\n",
      "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
      "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
      "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
      "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
      "  secondarysort: An example defining a secondary sort to the reduce.\n",
      "  sort: A map/reduce program that sorts the data written by the random writer.\n",
      "  sudoku: A sudoku solver.\n",
      "  teragen: Generate data for the terasort\n",
      "  terasort: Run the terasort\n",
      "  teravalidate: Checking results of terasort\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In deze notebook gaan we vooral focussen op het typische probleem van wordcount.\n",
    "Dit is een toepassing dat gaat tellen hoe vaak elk woord voorkomt in een bepaalde tekst.\n",
    "Om meer informatie over deze toepassing te krijgen kan je gebruik maken van het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In de eerste cell hebben we het input.txt bestand geupload. Of dit correct gebeurd is kan je controleren op de [file explorer van het hdfs](http://localhost:9870/explorer.html#/user/bigdata)\n",
    "\n",
    "Met onderstaande commando kan nu het precompiled word count example uitgevoerd worden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 08:56:34,619 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.7:8032\n",
      "2025-02-27 08:56:34,814 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.4:10200\n",
      "2025-02-27 08:56:35,106 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740574888337_0010\n",
      "2025-02-27 08:56:35,469 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2025-02-27 08:56:35,596 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-02-27 08:56:35,749 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1740574888337_0010\n",
      "2025-02-27 08:56:35,749 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-02-27 08:56:35,902 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-02-27 08:56:35,902 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-02-27 08:56:36,317 INFO impl.YarnClientImpl: Submitted application application_1740574888337_0010\n",
      "2025-02-27 08:56:36,369 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1740574888337_0010/\n",
      "2025-02-27 08:56:36,370 INFO mapreduce.Job: Running job: job_1740574888337_0010\n",
      "2025-02-27 08:56:41,511 INFO mapreduce.Job: Job job_1740574888337_0010 running in uber mode : false\n",
      "2025-02-27 08:56:41,512 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-02-27 08:56:46,565 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-02-27 08:56:50,585 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-02-27 08:56:50,591 INFO mapreduce.Job: Job job_1740574888337_0010 completed successfully\n",
      "2025-02-27 08:56:50,662 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=129\n",
      "\t\tFILE: Number of bytes written=559405\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=229\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6356\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14536\n",
      "\t\tTotal time spent by all map tasks (ms)=1589\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1817\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1589\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1817\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6508544\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14884864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=18\n",
      "\t\tMap output bytes=176\n",
      "\t\tMap output materialized bytes=121\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=15\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=121\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=81\n",
      "\t\tCPU time spent (ms)=730\n",
      "\t\tPhysical memory (bytes) snapshot=533504000\n",
      "\t\tVirtual memory (bytes) snapshot=13423042560\n",
      "\t\tTotal committed heap usage (bytes)=373817344\n",
      "\t\tPeak Map Physical memory (bytes)=266362880\n",
      "\t\tPeak Map Virtual memory (bytes)=5037559808\n",
      "\t\tPeak Reduce Physical memory (bytes)=267141120\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8385482752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=111\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordcount /user/bigdata/MapReduce/input.txt /user/bigdata/MapReduce/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na het uitvoeren van het wordcount applicatie is er op het HDFS een extra folder toegevoegd met als naam **output**.\n",
    "De plaats waar de output bewaard wordt is opgegeven in het bovenstaande commando en kan bekeken worden via de file-explorer van het hdfs.\n",
    "Deze folder bevat de volgende files:\n",
    "* Een _SUCCESS file dat leeg is. Deze file wordt gebruikt om aan te geven dat de MapReduce applicatie die resulteerde in de output goed afgerond was.\n",
    "* Een part-r-00000 file dat de output bevat. Indien de output te groot is kan het zijn dat deze file in meerdere files gesplitst wordt. Hier is dit niet het geval en bevat deze file alle ouput. De output van het bovenstaande commando zijn key-value paren waar de keys de verschillende woorden zijn en de values hoeveel keer elk woord voorkomt.\n",
    "\n",
    "De output van het commando kan dan met onderstaande code uitgelezen worden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\t1\n",
      "Dit\t1\n",
      "Hello\t1\n",
      "Wordcount\t1\n",
      "World,\t1\n",
      "een\t1\n",
      "file\t1\n",
      "hello\t2\n",
      "het\t1\n",
      "is\t1\n",
      "om\t1\n",
      "te\t1\n",
      "testen\t1\n",
      "voorbeeld\t2\n",
      "world,\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with client.read(map + '/output/part-r-00000') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs/process monitoring\n",
    "\n",
    "Bovestaande wordcount applicatie wordt uitgevoerd op YARN op de verschillende nodes van de cluster. Dit houdt in dat YARN gebruikt kan worden om de status van de applicatie op te volgen. Hiervoork kan je server naar [de webpagina van Yarn](http://localhost:8088). Na het klikken op de juiste status krijg je de correcte applicatie te zien. Hier kan je ook de logs bekijken door op logs te klikken.\n",
    "\n",
    "**LET OP: Doordat we in de webbrowser buiten de docker-containers zitten, kunnen we niet de historyserver hostname gebruiken. Als je een DNS-lookup error krijgt, vervang dan de domeinnaam/hostname door localhost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## OEFENING: MAPREDUCE \n",
    "\n",
    "Probeer nu op basis van bovenstaande reeds bestaande applicaties de gemiddelde lengte van de woorden in de text te bepalen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 09:16:29,750 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.7:8032\n",
      "2025-02-27 09:16:29,861 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.4:10200\n",
      "2025-02-27 09:16:30,084 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740574888337_0011\n",
      "2025-02-27 09:16:30,355 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2025-02-27 09:16:30,450 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-02-27 09:16:30,606 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1740574888337_0011\n",
      "2025-02-27 09:16:30,606 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-02-27 09:16:30,754 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-02-27 09:16:30,754 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-02-27 09:16:31,021 INFO impl.YarnClientImpl: Submitted application application_1740574888337_0011\n",
      "2025-02-27 09:16:31,107 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1740574888337_0011/\n",
      "2025-02-27 09:16:31,107 INFO mapreduce.Job: Running job: job_1740574888337_0011\n",
      "2025-02-27 09:16:37,200 INFO mapreduce.Job: Job job_1740574888337_0011 running in uber mode : false\n",
      "2025-02-27 09:16:37,201 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-02-27 09:16:42,247 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-02-27 09:16:47,287 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-02-27 09:16:47,294 INFO mapreduce.Job: Job job_1740574888337_0011 completed successfully\n",
      "2025-02-27 09:16:47,358 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=45\n",
      "\t\tFILE: Number of bytes written=559247\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=229\n",
      "\t\tHDFS: Number of bytes written=19\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10672\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16552\n",
      "\t\tTotal time spent by all map tasks (ms)=2668\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2069\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2668\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2069\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10928128\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16949248\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=36\n",
      "\t\tMap output bytes=522\n",
      "\t\tMap output materialized bytes=37\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=36\n",
      "\t\tCombine output records=2\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=37\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=126\n",
      "\t\tCPU time spent (ms)=1390\n",
      "\t\tPhysical memory (bytes) snapshot=486010880\n",
      "\t\tVirtual memory (bytes) snapshot=13425635328\n",
      "\t\tTotal committed heap usage (bytes)=349700096\n",
      "\t\tPeak Map Physical memory (bytes)=261451776\n",
      "\t\tPeak Map Virtual memory (bytes)=5038821376\n",
      "\t\tPeak Reduce Physical memory (bytes)=224559104\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8386813952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=111\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=19\n",
      "The mean is: 4.777777777777778\n"
     ]
    }
   ],
   "source": [
    "# uit te voeren op commando voor de gemiddelde lengte van de woorden te bekomen\n",
    "!hadoop jar hadoop-mapreduce-examples-3.3.6.jar wordmean /user/bigdata/MapReduce/input.txt /user/bigdata/MapReduce/output_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "De resource manager houdt ook een overzicht bij van de uitgevoerde applicaties, hun status, runtime en eventuele loggings. Na bovenstaande commando's uit te voeren moet je iets gelijkaardigs zien als in de onderstaande screenshot.\n",
    "\n",
    "![yarn of mapreduce](images/yarn_001.png)\n",
    "\n",
    "## Zelf implementeren van MapReduce applicaties\n",
    "\n",
    "Natuurlijk zijn er veel meer zaken mogelijk om te berekenen met map-reduce toepassingen dan de reeds gecompileerde in hadoop.\n",
    "Zoals eerder aangehaald valt vooral het coderen van de Map- en Reducestap hierbij op de schouders van de developer.\n",
    "De standaard programmeertaal van MapReduce is Java en dus ook het grootste deel van de documentatie over MapReduce is geschreven met behulp van Java.\n",
    "Deze programmas moeten dan gecompileerd worden tot een jar dat geupload kan worden naar de overeenkomstige nodes en daar uitgevoerd.\n",
    "De api overview van hadoop kan je [hier](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/package-summary.html) vinden.\n",
    "\n",
    "De code voor het wordcount example ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%file WordCount.java\n",
    "// dit schrijft onderstaande cell naar een file in de huidige directory\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "// Jullie gaan NOOIT moeten coderen in Java!!!!\n",
    "public class WordCount {\n",
    "\n",
    "    // Mapper klasse voor de map-functie te doen - tussen de <> - input-key, intput-value, output-key, output-value\n",
    "    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{\n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    "\n",
    "        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            // splits de input string in tokens (standaard op spatie splitsen)\n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            // itereer over alle woorden\n",
    "            while (itr.hasMoreTokens()){\n",
    "                // key is het huidige woord\n",
    "                word.set(itr.nextToken());\n",
    "                // emit key,value paar\n",
    "                context.write(word, one);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Reducer klasse voor de reduce-functie te doen - tussen de <> - input-key, intput-value, output-key, output-value\n",
    "    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable>{\n",
    "        private IntWritable result = new IntWritable();\n",
    "\n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "            int sum = 0;\n",
    "            for(IntWritable val: values){\n",
    "                sum += val.get();\n",
    "            }\n",
    "            result.set(sum);\n",
    "            context.write(key, result);\n",
    "        }\n",
    "    }\n",
    "\n",
    "   // configure the MapReduce program\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        Job job = Job.getInstance(conf, \"word count java\");\n",
    "        job.setJarByClass(WordCount.class);\n",
    "        // configure mapper\n",
    "        job.setMapperClass(TokenizerMapper.class);\n",
    "        // configure combiner (soort van reducer die draait op mapping node voor performantie)\n",
    "        job.setCombinerClass(IntSumReducer.class);\n",
    "        // configure reducer\n",
    "        job.setReducerClass(IntSumReducer.class);\n",
    "        // set output key-value classes\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    "        // set input file (first argument passed to the program)\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        // set output file  (second argument passed to the program)\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        // In this case, we wait for completion to get the output/logs and not stop the program to early.\n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Deze code bevat drie delen:\n",
    "* De main() functie: verzorgt de configuratie van de uit te voeren taak. Geeft aan wat de Map en Reduce klassen zijn, wat de input is, hoe de output bewaard wordt ,...\n",
    "* De Map-klasse met de map() functie bevat de code voor de mapping-fase\n",
    "* De Reduce-klasse met de reduce() functie bevat de code voor de reduce-fase\n",
    "\n",
    "De laatste twee klassen zijn hier gecodeerd als geneste klassen. Deze hadden ook in aparte files geplaatst kunnen worden.\n",
    "Nu moet deze code eerst omgezet/gecompileerd worden tot een .jar file. Deze kan dan analoog uitgevoerd worden als hierboven met het voorbeeldcode.\n",
    "Deze twee stappen kunnen uitgevoerd worden door onderstaande commando's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added manifest\n",
      "adding: WordCount$IntSumReducer.class(in = 1739) (out= 738)(deflated 57%)\n",
      "adding: WordCount$TokenizerMapper.class(in = 1736) (out= 753)(deflated 56%)\n",
      "adding: WordCount.class(in = 1496) (out= 818)(deflated 45%)\n"
     ]
    }
   ],
   "source": [
    "# compileren tot .jar\n",
    "!javac -cp \"hadoop-common-3.3.6.jar:hadoop-mapreduce-client-core-3.3.6.jar\" -d . WordCount.java\n",
    "!jar cvf wordcounter.jar *.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 09:32:05,332 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.7:8032\n",
      "2025-02-27 09:32:05,482 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.4:10200\n",
      "2025-02-27 09:32:05,663 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
      "2025-02-27 09:32:05,679 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740574888337_0012\n",
      "2025-02-27 09:32:05,953 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2025-02-27 09:32:06,043 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-02-27 09:32:06,177 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1740574888337_0012\n",
      "2025-02-27 09:32:06,177 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-02-27 09:32:06,308 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-02-27 09:32:06,308 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-02-27 09:32:06,580 INFO impl.YarnClientImpl: Submitted application application_1740574888337_0012\n",
      "2025-02-27 09:32:06,617 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1740574888337_0012/\n",
      "2025-02-27 09:32:06,618 INFO mapreduce.Job: Running job: job_1740574888337_0012\n",
      "2025-02-27 09:32:11,692 INFO mapreduce.Job: Job job_1740574888337_0012 running in uber mode : false\n",
      "2025-02-27 09:32:11,693 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-02-27 09:32:16,784 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-02-27 09:32:21,816 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-02-27 09:32:21,824 INFO mapreduce.Job: Job job_1740574888337_0012 completed successfully\n",
      "2025-02-27 09:32:21,892 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=129\n",
      "\t\tFILE: Number of bytes written=558917\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=229\n",
      "\t\tHDFS: Number of bytes written=111\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8320\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15344\n",
      "\t\tTotal time spent by all map tasks (ms)=2080\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1918\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2080\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1918\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8519680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15712256\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=18\n",
      "\t\tMap output bytes=176\n",
      "\t\tMap output materialized bytes=121\n",
      "\t\tInput split bytes=118\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=15\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=121\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=30\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=190\n",
      "\t\tCPU time spent (ms)=1170\n",
      "\t\tPhysical memory (bytes) snapshot=489861120\n",
      "\t\tVirtual memory (bytes) snapshot=13423554560\n",
      "\t\tTotal committed heap usage (bytes)=352845824\n",
      "\t\tPeak Map Physical memory (bytes)=262664192\n",
      "\t\tPeak Map Virtual memory (bytes)=5039112192\n",
      "\t\tPeak Reduce Physical memory (bytes)=227196928\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8384442368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=111\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=111\n"
     ]
    }
   ],
   "source": [
    "# execute\n",
    "!hadoop jar wordcounter.jar WordCount /user/bigdata/MapReduce/input.txt /user/bigdata/MapReduce/output_java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementeren via Python\n",
    "\n",
    "Hoewel het Hadoop Ecosysteem geprogrammeerd is in Java en het dus er goed mee samenwerkt, is het niet verplicht om Java te gebruiken om MapReduce applicaties te schrijven.\n",
    "Java krijgt namelijk veel kritiek, vooral doordat er veel code nodig is om eenvoudige zaken te programmeren.\n",
    "Om andere programmeertalen te gebruiken worden er verscheidene API's aangeboden door Hadoop, namelijk\n",
    "* Hadoop Streaming\n",
    "    * Communicatie via stdin/stdout\n",
    "    * Gebruikt door hadoopy, mrjob, ...\n",
    "* Hadoop Pipes\n",
    "    * C++ interface voor Hadoop\n",
    "    * Communicatie via sockets\n",
    "    * Gebruikt door pydoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mrjob\n",
    "\n",
    "De eerste API die we bekijken is MrJobs.\n",
    "Deze API maakt gebruik van de Hadoop Streaming API.\n",
    "De voordelen van MrJobs zijn:\n",
    "* Uitgebreidde documentatie\n",
    "* Code kan lokaal uitgevoerd worden als test\n",
    "* Data serialisatie gebeurt automatisch (nadeel van Streaming API)\n",
    "* Werkt met Amazon Elastic MapReduce en Google Cloud Dataproc\n",
    "\n",
    "Het grootste nadeel is dat de StreamingAPI niet de volledige kracht heeft van het Hadoop ecosysteem omdat alles omgezet wordt naar strings (jsons)\n",
    "\n",
    "Installatie van deze package gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_mrjob.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount_mrjob.py\n",
    "from mrjob.job import MRJob # importeer de mr job package\n",
    "\n",
    "class MRWordCount(MRJob): # maak een mapreduce applicatie aan\n",
    "    #words = {}\n",
    "    def mapper(self, _, line): \n",
    "        # hier schrijf je de code voor te mappen -> dit wordt lijn per lijn opgeroepen\n",
    "        # dit kan op verschillende nodes uitgevoerd worden voor verschillende blokken -> dus je gaat niet zomaar alles kunnen optellen hier\n",
    "        for word in line.split():\n",
    "            # onderstaande kan je niet doen\n",
    "            #words[word] += 1\n",
    "            yield (word, 1) # key-value paar om te emitten\n",
    "            # yield is een return die de code niet stopt\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze code kan uitgevoerd worden door het commando hieronder.\n",
    "Er zijn twee belangrijke parameters om toe te voegen aan het commando.\n",
    "* -r: Deze parameter geeft aan welke file uitgelezen wordt. Dit kan een lokale file zijn of een file op het hdfs. Om een hdfs-file aan te spreken gebruik je de volgende structuur: hdfs:///{path to file}\n",
    "* -o: Deze parameter geeft aan waar de data moet bewaard worden. Als deze parameter ontbreekt wordt het in de stdout geprint. Met de parameter kan je aangeven in welke file (lokaal of hdfs) de output moet bewaard worden.\n",
    "\n",
    "Meer informatie over hoe mrjob applicaties gestart kunnen worden vind je in [de documentatie](https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#running-your-job-different-ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/wordcount_mrjob.root.20250227.095036.934712\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/wordcount_mrjob.root.20250227.095036.934712/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/wordcount_mrjob.root.20250227.095036.934712/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar573065300816352834/] [] /tmp/streamjob1686846080262696588.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740649645876_0002\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1740649645876_0002\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1740649645876_0002\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1740649645876_0002/\n",
      "  Running job: job_1740649645876_0002\n",
      "  Job job_1740649645876_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1740649645876_0002 completed successfully\n",
      "  Output directory: hdfs:///user/bigdata/MapReduce/output_txt\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=141\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=140\n",
      "\t\tFILE: Number of bytes written=852177\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=377\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=141\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16039936\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14614528\n",
      "\t\tTotal time spent by all map tasks (ms)=3916\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15664\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1784\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14272\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3916\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1784\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1300\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=167\n",
      "\t\tInput split bytes=210\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=176\n",
      "\t\tMap output materialized bytes=146\n",
      "\t\tMap output records=18\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=268742656\n",
      "\t\tPeak Map Virtual memory (bytes)=5040021504\n",
      "\t\tPeak Reduce Physical memory (bytes)=265175040\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8385802240\n",
      "\t\tPhysical memory (bytes) snapshot=802279424\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce input records=18\n",
      "\t\tReduce output records=15\n",
      "\t\tReduce shuffle bytes=146\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=36\n",
      "\t\tTotal committed heap usage (bytes)=551026688\n",
      "\t\tVirtual memory (bytes) snapshot=18465665024\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bigdata/MapReduce/output_txt\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/wordcount_mrjob.root.20250227.095036.934712...\n",
      "Removing temp directory /tmp/wordcount_mrjob.root.20250227.095036.934712...\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount_mrjob.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt -o hdfs:///user/bigdata/MapReduce/output_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De bovenstaande manier vereist echter dat je steeds een python-file aanmaakt die dan via commandline gestart wordt.\n",
    "De reden hiervoor is is dat **de file naar de cluster verstuurd wordt**.\n",
    "Om de output te bekijken kan het output.txt bestand gedownload en uitgeprint worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"!\"\t1\n",
      "\"Dit\"\t1\n",
      "\"Hello\"\t1\n",
      "\"Wordcount\"\t1\n",
      "\"World,\"\t1\n",
      "\"een\"\t1\n",
      "\"file\"\t1\n",
      "\"hello\"\t2\n",
      "\"het\"\t1\n",
      "\"is\"\t1\n",
      "\"om\"\t1\n",
      "\"te\"\t1\n",
      "\"testen\"\t1\n",
      "\"voorbeeld\"\t2\n",
      "\"world,\"\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "map = 'MapReduce'\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "\n",
    "with client.read(map + '/output_txt/part-00000') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefeningen\n",
    "\n",
    "Maak de nodige mapreduce applicaties voor de volgende zaken te berekenen (je moet de output niet bewaren in een file):\n",
    "* De gemiddelde woordlengte\n",
    "* Het aantal keer dat elk karakter voorkomt\n",
    "* Het aantal woorden dat begint met elke letter\n",
    "* Het aantal woorden in de tekst\n",
    "\n",
    "Als een applicatie crasht, kan het zijn dat ze op de cluster nog een hele tijd actief blijft en het starten van nieuwe applicaties kan tegenhouden.\n",
    "Met onderstaande commando kan je een bestande applicatie afsluiten/killen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# killen van een applicatie\n",
    "yarn application -kill <Application_ID>\n",
    "# appliation id kan je vinden via de webbrowser van yarn (localhost:8088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vraag1.py\n"
     ]
    }
   ],
   "source": [
    "%%file vraag1.py\n",
    "# vraag 1: gemiddelde woordlengte\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class Vraag1(MRJob): \n",
    "    def mapper(self, _, line): \n",
    "        #lijn per lijn\n",
    "        for word in line.split():\n",
    "            # woord per woord\n",
    "            yield (\"lengte\", len(word)) # emit hoe lang elk woord is\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        aantal = 0\n",
    "        totaal = 0\n",
    "        for c in counts:\n",
    "            aantal += 1\n",
    "            totaal += c\n",
    "\n",
    "        if aantal == 0:\n",
    "            yield ('mean length', 0)\n",
    "        else:\n",
    "            yield ('mean length', totaal/aantal)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/vraag1.root.20250227.103811.670509\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/vraag1.root.20250227.103811.670509/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/vraag1.root.20250227.103811.670509/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4431078611934267120/] [] /tmp/streamjob7541794485209911823.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Error Launching job : Output directory hdfs://namenode:9000/user/bigdata/MapReduce/vraag1 already exists\n",
      "  Streaming Command Failed!\n",
      "Attempting to fetch counters from logs...\n",
      "Can't fetch history log; missing job ID\n",
      "No counters found\n",
      "Scanning logs for probable cause of failure...\n",
      "Can't fetch history log; missing job ID\n",
      "Can't fetch task logs; missing application ID\n",
      "Step 1 of 1 failed: Command '['/opt/hadoop-3.3.6/bin/hadoop', 'jar', '/opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar', '-files', 'hdfs:///user/root/tmp/mrjob/vraag1.root.20250227.103811.670509/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/root/tmp/mrjob/vraag1.root.20250227.103811.670509/files/wd/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/root/tmp/mrjob/vraag1.root.20250227.103811.670509/files/wd/vraag1.py#vraag1.py', '-input', 'hdfs:///user/bigdata/MapReduce/input.txt', '-output', 'hdfs:///user/bigdata/MapReduce/vraag1', '-mapper', '/bin/sh -ex setup-wrapper.sh python3 vraag1.py --step-num=0 --mapper', '-reducer', '/bin/sh -ex setup-wrapper.sh python3 vraag1.py --step-num=0 --reducer']' returned non-zero exit status 1280.\n"
     ]
    }
   ],
   "source": [
    "# test vraag 1\n",
    "!python3 vraag1.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt -o hdfs:///user/bigdata/MapReduce/vraag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vraag2.py\n"
     ]
    }
   ],
   "source": [
    "%%file vraag2.py\n",
    "# vraag 2: het aantal keer dat elk karakter voorkomt\n",
    "from mrjob.job import MRJob \n",
    "\n",
    "class Vraag2(MRJob):\n",
    "    def mapper(self, _, line): \n",
    "        for character in line:\n",
    "             yield (character, 1)\n",
    "         \n",
    "    def reducer(self, character, counts):\n",
    "        yield (character, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/vraag2.root.20250227.103628.066412\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/vraag2.root.20250227.103628.066412/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/vraag2.root.20250227.103628.066412/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3305501996250686879/] [] /tmp/streamjob806393535494912102.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740649645876_0004\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1740649645876_0004\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1740649645876_0004\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1740649645876_0004/\n",
      "  Running job: job_1740649645876_0004\n",
      "  Job job_1740649645876_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1740649645876_0004 completed successfully\n",
      "  Output directory: hdfs:///user/bigdata/MapReduce/vraag2\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=95\n",
      "\t\tFILE: Number of bytes written=851717\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=377\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=142\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16326656\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14221312\n",
      "\t\tTotal time spent by all map tasks (ms)=3986\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15944\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1736\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13888\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3986\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1736\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1310\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=134\n",
      "\t\tInput split bytes=210\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=600\n",
      "\t\tMap output materialized bytes=101\n",
      "\t\tMap output records=100\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=268136448\n",
      "\t\tPeak Map Virtual memory (bytes)=5042356224\n",
      "\t\tPeak Reduce Physical memory (bytes)=234881024\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8386781184\n",
      "\t\tPhysical memory (bytes) snapshot=765091840\n",
      "\t\tReduce input groups=23\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=23\n",
      "\t\tReduce shuffle bytes=101\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=200\n",
      "\t\tTotal committed heap usage (bytes)=617611264\n",
      "\t\tVirtual memory (bytes) snapshot=18469273600\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bigdata/MapReduce/vraag2\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/vraag2.root.20250227.103628.066412...\n",
      "Removing temp directory /tmp/vraag2.root.20250227.103628.066412...\n"
     ]
    }
   ],
   "source": [
    "# test vraag 2\n",
    "!python3 vraag2.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt -o hdfs:///user/bigdata/MapReduce/vraag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vraag3.py\n"
     ]
    }
   ],
   "source": [
    "%%file vraag3.py\n",
    "# vraag 3: Het aantal woorden dat begint met elke letter\n",
    "from mrjob.job import MRJob \n",
    "\n",
    "class Vraag3(MRJob): \n",
    "    def mapper(self, _, line): \n",
    "        for word in line.split():\n",
    "            if len(word)>0 and word[0].isalpha():\n",
    "                yield (word[0], 1) \n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts)) # dit blijft ongewijzigd hiervoor (gewoon optellen nodig)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/vraag3.root.20250227.103943.504529\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/vraag3.root.20250227.103943.504529/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/vraag3.root.20250227.103943.504529/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7283585460625571/] [] /tmp/streamjob7305549278336765119.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Error Launching job : Output directory hdfs://namenode:9000/user/bigdata/MapReduce/vraag3 already exists\n",
      "  Streaming Command Failed!\n",
      "Attempting to fetch counters from logs...\n",
      "Can't fetch history log; missing job ID\n",
      "No counters found\n",
      "Scanning logs for probable cause of failure...\n",
      "Can't fetch history log; missing job ID\n",
      "Can't fetch task logs; missing application ID\n",
      "Step 1 of 1 failed: Command '['/opt/hadoop-3.3.6/bin/hadoop', 'jar', '/opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar', '-files', 'hdfs:///user/root/tmp/mrjob/vraag3.root.20250227.103943.504529/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/root/tmp/mrjob/vraag3.root.20250227.103943.504529/files/wd/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/root/tmp/mrjob/vraag3.root.20250227.103943.504529/files/wd/vraag3.py#vraag3.py', '-input', 'hdfs:///user/bigdata/MapReduce/input.txt', '-output', 'hdfs:///user/bigdata/MapReduce/vraag3', '-mapper', '/bin/sh -ex setup-wrapper.sh python3 vraag3.py --step-num=0 --mapper', '-reducer', '/bin/sh -ex setup-wrapper.sh python3 vraag3.py --step-num=0 --reducer']' returned non-zero exit status 1280.\n"
     ]
    }
   ],
   "source": [
    "# test vraag 3\n",
    "!python3 vraag3.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt -o hdfs:///user/bigdata/MapReduce/vraag3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vraag4.py\n"
     ]
    }
   ],
   "source": [
    "%%file vraag4.py\n",
    "# vraag 4: Het aantal woorden in de tekst\n",
    "from mrjob.job import MRJob \n",
    "\n",
    "class Vraag4(MRJob): \n",
    "    def mapper(self, _, line): \n",
    "        for word in line.split():\n",
    "            yield (\"aantal woorden\", 1) \n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag4.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/vraag4.root.20250227.103741.474595\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/vraag4.root.20250227.103741.474595/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/vraag4.root.20250227.103741.474595/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar603351783940513249/] [] /tmp/streamjob8684510251235678188.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740649645876_0006\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1740649645876_0006\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1740649645876_0006\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1740649645876_0006/\n",
      "  Running job: job_1740649645876_0006\n",
      "  Job job_1740649645876_0006 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1740649645876_0006 completed successfully\n",
      "  Output directory: hdfs:///user/bigdata/MapReduce/vraag4\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=20\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=48\n",
      "\t\tFILE: Number of bytes written=851626\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=377\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=20\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16822272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15458304\n",
      "\t\tTotal time spent by all map tasks (ms)=4107\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16428\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1887\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15096\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4107\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1887\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1390\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=168\n",
      "\t\tInput split bytes=210\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=342\n",
      "\t\tMap output materialized bytes=54\n",
      "\t\tMap output records=18\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=265609216\n",
      "\t\tPeak Map Virtual memory (bytes)=5044015104\n",
      "\t\tPeak Reduce Physical memory (bytes)=270274560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8386920448\n",
      "\t\tPhysical memory (bytes) snapshot=799154176\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=18\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=54\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=36\n",
      "\t\tTotal committed heap usage (bytes)=562036736\n",
      "\t\tVirtual memory (bytes) snapshot=18470408192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/bigdata/MapReduce/vraag4\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/vraag4.root.20250227.103741.474595...\n",
      "Removing temp directory /tmp/vraag4.root.20250227.103741.474595...\n"
     ]
    }
   ],
   "source": [
    "# test vraag 4\n",
    "!python3 vraag4.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt -o hdfs:///user/bigdata/MapReduce/vraag4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Werken met MapReduce voor gestructureerde data\n",
    "\n",
    "Mapreduce kan ook gebruikt worden om te werken met gestructureerde data (bijvoorbeeld een csv) waar elke rij 1 data-element voorsteld.\n",
    "Het is hier echter wel belangrijk dat alle data op 1 lijn gecombineerd wordt dus multiline csv's, jsons of xml bestanden kunnen niet verwerkt worden.\n",
    "\n",
    "Onderstaande code is een voorbeeld voor hoe je een csv kan uitlezen en een aantal statistieken kan berekenen. Hierin leer je vooral:\n",
    "* Hoe de csv lijn per lijn te verwerken en kolommen te detecteren\n",
    "* Hoe de header rij eruit filteren\n",
    "* Hoe meerdere berekeningen op een iterator te doen\n",
    "\n",
    "We gebruiken hiervoor de titanic.csv file. Let op dat die naam van de passagier hierbij komma's kan bevatten dus dit vereist wat extra aandacht.\n",
    "Daarnaast gebruiken we de lokale versie van het bestand en niet de geuploadde versie.\n",
    "\n",
    "We willen de volgende zaken berekenen door middel van 1 map-reduce applicatie:\n",
    "* Gemiddelde leeftijd\n",
    "* Percentage overleeft\n",
    "* Percentage mannelijke passagiers\n",
    "* Percentage vrouwelijke passagiers die het overleefd hebben: 30% betekend dat 30% van de vrouwelijke passagiers het overleefd hebben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing structured.py\n"
     ]
    }
   ],
   "source": [
    "%%file structured.py\n",
    "from mrjob.job import MRJob \n",
    "import csv # om csv-based text om te zetten\n",
    "from io import StringIO\n",
    "\n",
    "col_age = 5\n",
    "col_survived = 1\n",
    "col_gender = 4\n",
    "\n",
    "class MR_structured(MRJob): \n",
    "    def mapper(self, _, line): \n",
    "        # col1, col2, col3, col4, ...\n",
    "\n",
    "        if line == \"PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\": # voeg dit toe om niet je headerrij te verwerken\n",
    "            return\n",
    "        \n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file)) # zet de csv-text om naar een lijst met kolommen\n",
    "        # op deze manier kunnen wordt het volgende ook correct omgezet test,\"hallo world, met komma\", test3\n",
    "        # 1 zaak om mee op te letten -> elke lijn van de csv 1 entry bevat -> sommige datasets bevatten tekst met newline characters\n",
    "            # -> die newline characters verwijder je best voor je het bestand oplaad naar het hdfs\n",
    "\n",
    "        if len(cols) != 12:\n",
    "            return # indien er iets zou misgegaan zijn met het parsen/lege rijen zou tegenkomen\n",
    "\n",
    "        # gemiddelde leeftijd\n",
    "        if cols[col_age] != '': # vermijd null-waarden problemen\n",
    "            yield ('age', float(cols[col_age])) # zet de string ook om naar een getal\n",
    "\n",
    "        if cols[col_survived] != '':\n",
    "            yield('overleefd', int(cols[col_survived]))\n",
    "            \n",
    "        if cols[col_gender] != '':\n",
    "            yield('geslacht', (cols[col_gender]))\n",
    "            \n",
    "        if cols[col_gender] != 'female' and cols[col_survived] != '':\n",
    "            yield('vrouw_overleefd', int(cols[col_survived])) # wat we hier emitten is de volledige survived kolom voor vrouwen\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        if key == 'age':\n",
    "            counts = list(counts)\n",
    "            aantal = len(counts)\n",
    "            totaal = sum(counts) # als je het niat omzet naar een list gaat deze lijna een fout geven -> counts is een iterator -> kan niet herstarten\n",
    "\n",
    "            yield ('mean age', totaal/aantal)\n",
    "\n",
    "        elif key == 'overleefd' or key == 'vrouw_overleefd':\n",
    "            counts = list(counts)\n",
    "            aantal = len(counts)\n",
    "            totaal = sum(counts)\n",
    "\n",
    "            yield (key, totaal/aantal*100.0)\n",
    "\n",
    "        elif key == 'geslacht':\n",
    "            aantal = 0\n",
    "            for gender in counts:\n",
    "                if gender == 'male':\n",
    "                    aantal_mannen += 1\n",
    "                aantal += 1\n",
    "            yield ('percentage mannen', aantal_mannen/aantal*100.0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR_structured.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/structured.root.20250227.105846.025241\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/structured.root.20250227.105846.025241/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/structured.root.20250227.105846.025241/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3672087349239166291/] [] /tmp/streamjob3257840917041114031.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "  Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1740649645876_0007\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1740649645876_0007\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1740649645876_0007\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1740649645876_0007/\n",
      "  Running job: job_1740649645876_0007\n",
      "  Job job_1740649645876_0007 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "  Task Id : attempt_1740649645876_0007_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "  Task Id : attempt_1740649645876_0007_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "  Task Id : attempt_1740649645876_0007_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "   map 100% reduce 100%\n",
      "  Job job_1740649645876_0007 failed with state FAILED due to: Task failed task_1740649645876_0007_m_000001\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "  Job not successful!\n",
      "  Streaming Command Failed!\n",
      "Counters: 42\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=283932\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=161\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=3\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=37888000\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=55992320\n",
      "\t\tTotal time spent by all map tasks (ms)=9250\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=37000\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6835\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=54680\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9250\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6835\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=420\n",
      "\t\tCombine input records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=68\n",
      "\t\tInput split bytes=105\n",
      "\t\tMap input records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=14\n",
      "\t\tMap output records=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPeak Map Physical memory (bytes)=268001280\n",
      "\t\tPeak Map Virtual memory (bytes)=5040398336\n",
      "\t\tPhysical memory (bytes) snapshot=268001280\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=183500800\n",
      "\t\tVirtual memory (bytes) snapshot=5040398336\n",
      "Scanning logs for probable cause of failure...\n",
      "Looking for history log in hdfs:///tmp/hadoop-yarn/staging...\n",
      "Looking for history log in /opt/hadoop-3.3.6/logs...\n",
      "\n",
      "Probable cause of failure:\n",
      "\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:350)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
      "\n",
      "\n",
      "Step 1 of 1 failed: Command '['/opt/hadoop-3.3.6/bin/hadoop', 'jar', '/opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar', '-files', 'hdfs:///user/root/tmp/mrjob/structured.root.20250227.105846.025241/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/root/tmp/mrjob/structured.root.20250227.105846.025241/files/wd/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/root/tmp/mrjob/structured.root.20250227.105846.025241/files/wd/structured.py#structured.py', '-input', 'hdfs:///user/bigdata/MapReduce/input.txt', '-output', 'hdfs:///user/bigdata/MapReduce/structured', '-mapper', '/bin/sh -ex setup-wrapper.sh python3 structured.py --step-num=0 --mapper', '-reducer', '/bin/sh -ex setup-wrapper.sh python3 structured.py --step-num=0 --reducer']' returned non-zero exit status 256.\n"
     ]
    }
   ],
   "source": [
    "# test structured\n",
    "!python3 structured.py -r hadoop hdfs:///user/bigdata/MapReduce/input.txt -o hdfs:///user/bigdata/MapReduce/structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "Indien er een python script voor een map-reduce applicatie een error bevat is het niet altijd eenvoudig om de specifieke foutmelding te vinden. Dit is vooral merkbaar als we een runtime errors.\n",
    "Indien we in het voorgaande script de beveiligingen rond het casten van empty strings verwijderen. Dan krijgen we foutmeldingen. Indien we ditzelfde nu uitvoeren (maar uitvoeren op de cluster in plaats van lokaal in tegenstelling tot het voorgaande commando), dan krijgen we geen duidelijke foutmelding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting structured_error.py\n"
     ]
    }
   ],
   "source": [
    "%%file structured_error.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "col_survived = 1\n",
    "col_sex = 4\n",
    "col_age = 5\n",
    "\n",
    "class MR_Structured(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line == \"PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\":\n",
    "            return\n",
    "        \n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file))    \n",
    "\n",
    "        yield (\"age\", float(cols[col_age]))\n",
    "        yield (\"overleefd\", int(cols[col_survived]))\n",
    "        yield (\"geslacht\" , cols[col_sex])\n",
    "        yield (\"vrouw_overleefd\", int(cols[col_survived]))\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "\n",
    "        if word == \"age\":\n",
    "            counts = list(counts)\n",
    "            total = sum(counts)\n",
    "            size=len(counts)\n",
    "            yield (word, total/size)\n",
    "        elif word == \"overleefd\" or word == \"vrouw_overleefd\":\n",
    "            counts = list(counts)\n",
    "            total = sum(counts)\n",
    "            size=len(counts)\n",
    "            yield (word, total/size*100.0)\n",
    "        elif word == \"geslacht\": \n",
    "            aantal_mannen = 0\n",
    "            aantal_vrouwen = 0\n",
    "            for gender in counts:\n",
    "                if gender == \"male\":\n",
    "                    aantal_mannen += 1\n",
    "                else:\n",
    "                    aantal_vrouwen += 1\n",
    "                    \n",
    "            yield (\"percentage mannen\", aantal_mannen/(aantal_mannen + aantal_vrouwen) * 100.0)\n",
    "        else:\n",
    "            yield(\"dummy\", next(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR_Structured.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/structured_error.root.20250227.110623.045914\n",
      "Running step 1 of 1...\n",
      "\n",
      "Error while reading from /tmp/structured_error.root.20250227.110623.045914/step/000/mapper/00000/input:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/workspace_groep123/Week_03/structured_error.py\", line 51, in <module>\n",
      "    MR_Structured.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/job.py\", line 616, in run\n",
      "    cls().execute()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/job.py\", line 687, in execute\n",
      "    self.run_job()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/job.py\", line 636, in run_job\n",
      "    runner.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/runner.py\", line 503, in run\n",
      "    self._run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 161, in _run\n",
      "    self._run_step(step, step_num)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 170, in _run_step\n",
      "    self._run_streaming_step(step, step_num)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 181, in _run_streaming_step\n",
      "    self._run_mappers_and_combiners(step_num, map_splits)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 219, in _run_mappers_and_combiners\n",
      "    self._run_multiple(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 130, in _run_multiple\n",
      "    func()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 724, in _run_mapper_and_combiner\n",
      "    run_mapper()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/sim.py\", line 746, in _run_task\n",
      "    invoke_task(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/inline.py\", line 133, in invoke_task\n",
      "    task.execute()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/job.py\", line 675, in execute\n",
      "    self.run_mapper(self.options.step_num)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/job.py\", line 760, in run_mapper\n",
      "    for k, v in self.map_pairs(read_lines(), step_num=step_num):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/mrjob/job.py\", line 830, in map_pairs\n",
      "    for k, v in mapper(key, value) or ():\n",
      "  File \"/home/bigdata/workspace_groep123/Week_03/structured_error.py\", line 20, in mapper\n",
      "    yield (\"age\", float(cols[col_age]))\n",
      "ValueError: could not convert string to float: ''\n"
     ]
    }
   ],
   "source": [
    "!python3 structured_error.py titanic.csv\n",
    "# hiermee krijg je onmiddelijk de foutmelding -> er is geen -o gedefinieerd -> schrijf de output hier\n",
    "# het kan ook voor te testen handiger zijn om -r hadoop weg te laten en eerst lokaal te runnen\n",
    "# bij de type A/B evaluaties is het niet de bedoeling om het lokaal te runnen maar in de cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na het uitvoeren van bovenstaande script, dan zie je dat de applicatie gestart wordt (map 0% reduce 0%) maar een duideljke foutmelding is er niet (run time exception).\n",
    "Om de logs te bekijken kan je \n",
    "* surfen gaan naar de historyserver die beschikbaar is op de url: localhost:8188\n",
    "* het volgende command-line commando uitvoeren. Dit haalt alle std-err logs van alle containers op. Zoek naar een python error en je zal vinden wat er misgaat.\n",
    "* een python script schrijven om de yarn log-server te bevragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yarn logs -applicationId {app id} | sed -n '/LogType:stderr/,/End of LogType:stderr/p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 11:08:28,100 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.18.0.5:8032\n",
      "2025-02-27 11:08:28,232 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.6:10200\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:1731\n",
      "LogContents:\n",
      "Feb 27, 2025 10:59:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver as a provider class\n",
      "Feb 27, 2025 10:59:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\n",
      "Feb 27, 2025 10:59:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices as a root resource class\n",
      "Feb 27, 2025 10:59:02 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\n",
      "INFO: Initiating Jersey application, version 'Jersey: 1.19.4 05/24/2017 03:20 PM'\n",
      "Feb 27, 2025 10:59:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "Feb 27, 2025 10:59:03 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "Feb 27, 2025 10:59:03 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices to GuiceManagedComponentProvider with the scope \"PerRequest\"\n",
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.mapreduce.v2.app.MRAppMaster).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "\n",
      "End of LogType:stderr\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:1780\n",
      "LogContents:\n",
      "+ __mrjob_PWD=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007\n",
      "+ exec\n",
      "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "+ export PYTHONPATH=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/mrjob.zip:\n",
      "+ exec\n",
      "+ cd /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007\n",
      "+ python3 structured.py --step-num=0 --mapper\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/structured.py\", line 56, in <module>\n",
      "    MR_structured.run()\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/mrjob.zip/mrjob/job.py\", line 616, in run\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/mrjob.zip/mrjob/job.py\", line 675, in execute\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/mrjob.zip/mrjob/job.py\", line 760, in run_mapper\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/mrjob.zip/mrjob/job.py\", line 830, in map_pairs\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000007/structured.py\", line 20, in mapper\n",
      "    if cols[col_age] != '': # vermijd null-waarden problemen\n",
      "IndexError: list index out of range\n",
      "\n",
      "End of LogType:stderr\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:1780\n",
      "LogContents:\n",
      "+ __mrjob_PWD=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006\n",
      "+ exec\n",
      "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "+ export PYTHONPATH=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/mrjob.zip:\n",
      "+ exec\n",
      "+ cd /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006\n",
      "+ python3 structured.py --step-num=0 --mapper\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/structured.py\", line 56, in <module>\n",
      "    MR_structured.run()\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/mrjob.zip/mrjob/job.py\", line 616, in run\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/mrjob.zip/mrjob/job.py\", line 675, in execute\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/mrjob.zip/mrjob/job.py\", line 760, in run_mapper\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/mrjob.zip/mrjob/job.py\", line 830, in map_pairs\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000006/structured.py\", line 20, in mapper\n",
      "    if cols[col_age] != '': # vermijd null-waarden problemen\n",
      "IndexError: list index out of range\n",
      "\n",
      "End of LogType:stderr\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:0\n",
      "LogContents:\n",
      "\n",
      "End of LogType:stderr\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:1780\n",
      "LogContents:\n",
      "+ __mrjob_PWD=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004\n",
      "+ exec\n",
      "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "+ export PYTHONPATH=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/mrjob.zip:\n",
      "+ exec\n",
      "+ cd /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004\n",
      "+ python3 structured.py --step-num=0 --mapper\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/structured.py\", line 56, in <module>\n",
      "    MR_structured.run()\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/mrjob.zip/mrjob/job.py\", line 616, in run\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/mrjob.zip/mrjob/job.py\", line 675, in execute\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/mrjob.zip/mrjob/job.py\", line 760, in run_mapper\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/mrjob.zip/mrjob/job.py\", line 830, in map_pairs\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000004/structured.py\", line 20, in mapper\n",
      "    if cols[col_age] != '': # vermijd null-waarden problemen\n",
      "IndexError: list index out of range\n",
      "\n",
      "End of LogType:stderr\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:1780\n",
      "LogContents:\n",
      "+ __mrjob_PWD=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003\n",
      "+ exec\n",
      "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "+ export PYTHONPATH=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/mrjob.zip:\n",
      "+ exec\n",
      "+ cd /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003\n",
      "+ python3 structured.py --step-num=0 --mapper\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/structured.py\", line 56, in <module>\n",
      "    MR_structured.run()\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/mrjob.zip/mrjob/job.py\", line 616, in run\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/mrjob.zip/mrjob/job.py\", line 675, in execute\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/mrjob.zip/mrjob/job.py\", line 760, in run_mapper\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/mrjob.zip/mrjob/job.py\", line 830, in map_pairs\n",
      "  File \"/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000003/structured.py\", line 20, in mapper\n",
      "    if cols[col_age] != '': # vermijd null-waarden problemen\n",
      "IndexError: list index out of range\n",
      "\n",
      "End of LogType:stderr\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Thu Feb 27 10:59:26 +0000 2025\n",
      "LogLength:551\n",
      "LogContents:\n",
      "+ __mrjob_PWD=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000002\n",
      "+ exec\n",
      "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
      "+ export PYTHONPATH=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000002/mrjob.zip:\n",
      "+ exec\n",
      "+ cd /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1740649645876_0007/container_e04_1740649645876_0007_01_000002\n",
      "+ python3 structured.py --step-num=0 --mapper\n",
      "\n",
      "End of LogType:stderr\n"
     ]
    }
   ],
   "source": [
    "!yarn logs -applicationId application_1740649645876_0007 | sed -n '/LogType:stderr/,/End of LogType:stderr/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afsluitende opmerkingen\n",
    "\n",
    "Het is belangrijk om te beseffen dat deze applicatie op meerdere nodes kan tegelijkertijd uitgevoerd worden. \n",
    "Dit heeft als gevolg dat je geen persistente counters kan toevoegen in de reducer om gemiddelden en dergelijke te berekenen.\n",
    "Een globaal overzicht van het dataframe kan maar **in de mapper** behaald worden.\n",
    "\n",
    "Daarnaast is er nog een andere stap mogelijk dan mapper of reducer. Dat is **de combiner** stap. Dit is een stap die runt per node en gebruikt kan worden om al wat combinaties te doen zodat er minder data tussen nodes moet verstuurd worden. In grote applicaties/datasets kan dit heel wat internettrafiek besparen wat de werking van de cluster ten goede komt.\n",
    "\n",
    "Ten derde is het ook mogelijk om meerdere stappen te definieren in de mrjob applicatie. Dit kan door de steps functie te implementeren en daar elke stap in te definieren. Elke stap hierin kan beschikken over een eigen mapper/reducer en combiner functie. Meer informatie vind je [in de documentatie](https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#writing-your-second-job)\n",
    "\n",
    "## Verdere oefeningen\n",
    "\n",
    "Gebruik nu de titanic csv om met behulp van een mapreduce applicatie de volgende zaken te berekenen:\n",
    "* Het aantal unieke plaatsen waar personen aan boord zijn gekomen (embarked kolom)\n",
    "* Het aantal ontbrekende waarden in de Cabin kolom\n",
    "* De volgende statistische waarden voor de ticketprijs (Fare) kolom: min, max, mean, std\n",
    "* De langste naam van een passagier\n",
    "* Het aantal passagiers per klasse\n",
    "* Het totaal aantal passagiers op de titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file structured_oefening.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
