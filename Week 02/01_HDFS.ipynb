{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS\n",
    "\n",
    "HDFS is het distributed file system van Hadoop dat de basis vormt voor een breed gamma van applicaties, waaronder MapReduce.\n",
    "Dit framework maakt het mogelijk om niet-gespecialiseerde hardware te gebruiken om eenvoudig datacenters op te zetten of rekenclusters te beheren.\n",
    "HDFS bereikt dit doel op een aantal manieren:\n",
    "* Ten eerste wordt een file opgedeeld in verschillende blokken. Deze worden verdeeld over verschillende computers zodat code meerdere delen van het bestand kan gebruiken in parallel om zo de benodigde rekenkracht te verdelen over meerdere toestellen.\n",
    "* Daarnaast worden de blokken ook gedupliceerd om zo fout-toleranter te zijn in het geval van een crash (hardware of software), stroomstoring, netwerkonderbreking.\n",
    "\n",
    "Om dit te bereiken gebruikt Hadoop een Master-Slave architectuur dat bestaat uit een enkele namenode (master) en meerdere datanodes (slaves).\n",
    "De namenode houdt bij hoeveel datanodes er actief zijn, welke blokken ze hebben en welke blokken bij welke file horen.\n",
    "Indien er een datanode crasht gaat deze server dit detecteren en dit oplossen door de nodige blokken te kopieren van een andere datanode zodat er steeds voldoende kopies in het systeem aanwezig zijn.\n",
    "Bij elke actie die uitgevoerd moet worden in het HDFS moet er steeds gevraagd worden aan de namenode welke blokken op welke datanodes we nodig hebben voor de gewenste file uit te lezen of code voor uit te voeren.\n",
    "Het is dus duidelijk dat deze namenode een single-point-of-failure is wat ideaal is voor de availability van de cluster.\n",
    "Dit kan opgelost worden door HDFS te runnen in een high-availability mode wat ervoor zorgt dat er steeds een backup aanwezig is voor de namenode die zeer snel de werking kan overnemen.\n",
    "Deze structuur maakt het eenvoudig om aan horizontal scaling te doen door extra servers toe te voegen zonder dat er downtime is voor de hele cluster.\n",
    "\n",
    "Dit resulteer in de volgende kenmerken van HDFS:\n",
    "* High Throughput\n",
    "* Scalability\n",
    "* High Availability\n",
    "* Data Reliability\n",
    "* Fault Tolerance\n",
    "\n",
    "## Starten en stoppen van de cluster\n",
    "\n",
    "De cluster kan gestart worden door alle docker containers te starten die gedefinieerd worden in de yaml-file.\n",
    "Dit kan via de docker desktop gui of via de command line door middel van het docker-compose commando.\n",
    "Stoppen kan dan via dezelfde methodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Communiceren met het HDFS\n",
    "\n",
    "Er zijn verschillende manieren om dit distributed bestandssysteem te gebruiken.\n",
    "Ten eerste kan je gebruik maken van een command line interface (CLI) om bestanden uit te lezen, op te laden, ...\n",
    "Daarnaast zijn er ook wrappers voor verschillende talen die toelaten om rechtstreeks vanuit code te interageren met het HDFS.\n",
    "De voordelen van deze wrappers over een CLI zijn:\n",
    "* Flexibiler om te interageren in applicaties.\n",
    "* Gebruiksvriendelijker en gemakkelijker om te automatiseren.\n",
    "* Eenvoudiger in onderhoud en te debuggen\n",
    "* Volledige functionaliteit van een programmeertaal kan gebruikt worden zoals OO.\n",
    "\n",
    "### Instantiering van een client\n",
    "\n",
    "Veel verschillende talen beschikken over een wrapper om te communiceren met een HDFS.\n",
    "In deze cursus gaan we gebruik maken van [InsecureClient in hdfscli](https://hdfscli.readthedocs.io/en/latest/quickstart.html) wrapper.\n",
    "De eerste stap in het gebruik van de wrapper is te bepalen hoe de namenode van het hdfs gevonden kan worden.\n",
    "Dit gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "# connecteer met de namenode\n",
    "client = InsecureClient(\"http://localhost:9870\", user='bigdata') \n",
    "\n",
    "# 1 opmerking voor het werken met jupyter lab -> er is een rudimentaire vorm van intellisense (je kan de tab-toets gebruiken voor aan te vullen)\n",
    "# je krijgt niet zoveel tips als in visual studio code om code te schrijven\n",
    "# zorg dat je zeker het goed oefent, zodat je zeker de basis kent\n",
    "\n",
    "if client.status('/bla', strict=False): # zonder False krijg je een foutmelding\n",
    "    print('Path exists')\n",
    "else:\n",
    "    print('Path does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via cli\n",
    "!hdfs dfs -test -d /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aanmaken van files en directories\n",
    "\n",
    "Om nu bestanden en folders aan te maken op dit distributed file systeem kunnen onderstaande functies gebruikt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if client.status('hdfs', strict=False) is None: # dit gaat het pad /user/bigdata/hdfs controleren\n",
    "    client.makedirs('hdfs')\n",
    "    print('directory created')\n",
    "\n",
    "# zelfde als hierboven\n",
    "!hdfs dfs -mkdir -p /user/bigdata/hdfscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make files\n",
    "\n",
    "# client.upload(pad cluster, lokaal pad)\n",
    "#client.upload('hdfs', 'davinci_notebooks.txt')\n",
    "#client.upload('hdfs', 'outline_of_science.txt')\n",
    "#client.upload('hdfs', 'ulysses.txt')\n",
    "client.upload('hdfs/oudeboek.txt', 'ulysses.txt', overwrite =True, replication=2, blocksize=1048576)\n",
    "client.write('hdfs/oudeboek2.txt', 'ulysses.txt') # kan meer zaken gaan schrijven dan files (bvb in dit geval de gewone string ulysses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hadoop fs -put /path/in/linux /hdfs/path\n",
    "# hadoop fs -get /hdfs/path /path/in/linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bekijken van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hdfs dfs -ls command\n",
    "print(client.acl_status('hdfs'))\n",
    "print(client.checksum('hdfs/oudeboek.txt')) # om de authenticiteit van het bestand te verifieren\n",
    "print(client.content('hdfs/oudeboek.txt'))\n",
    "print(client.list('hdfs')) # dit is de ls\n",
    "\n",
    "print(client.walk('/')) # itereer over alle bestanden, duik ook in subfolders\n",
    "for f in client.walk('/'):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitlezen van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.download('hdfs/davinci_notebooks.txt', 'notebooks.txt', overwrite=True)\n",
    "\n",
    "# meer gericht gaan bekijken of het in de code gaan bewerken\n",
    "with client.read('hdfs/oudeboek2.txt') as reader: # dit bestand was met write gedaan ipv met upload\n",
    "    content = reader.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aanpassen van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.delete('hdfs/oudeboek.txt', recursive=False) # recursive -> om folders te verwijderen\n",
    "# hdfs dfs -rm path\n",
    "# add -r tag for deleting directory\n",
    "\n",
    "# hadoop fs -mv oldname newname\n",
    "#client.rename('hdfs/oudeboek2.txt', 'hdfs/oudeboek3.txt')\n",
    "\n",
    "client.set_permission('hdfs/oudeboek3.txt', 777)\n",
    "# hdfs dfs –setrep –w 3 /tmp/logs/file.txt\n",
    "client.set_replication('hdfs/oudeboek3.txt', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Los de volgende oefeningen op:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat de informatie van een bestand in HDFS ophaalt en weergeeft. De informatie moet onder andere de grootte van het bestand, de eigenaar, de groep en de permissies bevatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat de volledige inhoud van een tekstbestand in HDFS leest en afdrukt naar de console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat een tekstbestand van de lokale schijf naar HDFS schrijft. Het script moet de inhoud van het lokale bestand lezen en deze naar een nieuw bestand in HDFS schrijven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat de permissies van een bestand in HDFS wijzigt. Het script moet de permissies instellen op een door de gebruiker opgegeven waarde (bijvoorbeeld 755)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat de huidige replicatiefactor van een bestand in HDFS controleert en weergeeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat alle bestanden in een HDFS-directory zoekt die voldoen aan een bepaald naamspatroon (bijvoorbeeld alle .txt bestanden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat nieuwe inhoud toevoegt aan een bestaand bestand in HDFS. Het script moet de nieuwe inhoud aan het einde van het bestand toevoegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf een Python-script dat de checksum van een bestand in HDFS ophaalt en weergeeft. Dit kan nuttig zijn om de integriteit van bestanden te controleren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Schrijf python code dat controleert of een directory bestaat in het hdfs.\n",
    "Indien nee wordt de directory aangemaakt.\n",
    "Indien ja, worden alle files in de directory verwijderd om van een lege directory te starten.\n",
    "Upload daarna een tekst-bestand naar keuze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_oefening = 'hdfs'\n",
    "if client.status(directory_oefening, strict=False) is None:\n",
    "    # de directory bestaat niet -> maak ze aan\n",
    "    client.makedirs(directory_oefening)\n",
    "else:\n",
    "    # de directory bestaat -> zorgen dat ze leeg is\n",
    "    for f in client.list(directory_oefening):\n",
    "        print(f)\n",
    "        client.delete(directory_oefening + '/' + f, recursive=False) # verwijder alles dat in de directory zit (kan complexer gemaakt worden door eventuele bestanden niet te verwijderen)\n",
    "\n",
    "client.upload(directory_oefening, 'notebooks.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
